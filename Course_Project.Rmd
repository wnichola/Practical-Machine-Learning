---
title: "Practical Machine Learning: Course Project"
author: "Nicholas Wee"
date: "Saturday, June 13, 2015"
output:
  html_document:
    keep_md: yes
---

## Download and load files into data.tables
Check if the data files exist.  If not, download it from the URL.  When reading in the file, change invalid values to NA.  

```{r}
library(caret)
setInternet2(TRUE)
setwd("C:/Data/My Project/MOCC/03 Quiz and Projects/08 Machine Learning")

target <- "pml_training.csv"
if (!file.exists(target)) {
    url <-
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    target <- "pml_training.csv"
    download.file(url, destfile = target)
}
training <- read.csv(target, na.strings = c("NA","#DIV/0!",""))

target <- "pml_testing.csv"
if (!file.exists(target)) {
    url <-
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, destfile = target)
}
testing <- read.csv(target, na.strings = c("NA","#DIV/0!",""))
```

## Remove invalid predictors
Reduce the number of predictors by removing columns that have near zero values, NA, or is empty.

```{r}
# Remove columns with Near Zero Values
subTrain <-
    training[, names(training)[!(nzv(training, saveMetrics = T)[, 4])]]

# Remove columns with NA or is empty
subTrain <-
    subTrain[, names(subTrain)[sapply(subTrain, function (x)
        ! (any(is.na(x) | x == "")))]]


# Remove V1 which seems to be a serial number, and
# cvtd_timestamp that is unlikely to influence the prediction
subTrain <- subTrain[,-1]
subTrain <- subTrain[, c(1:3, 5:58)]
```
## Separate the data to be used for Cross Validation
Using the training data, separate out a set to be used for validation.  From what I've read, there is no need to create a separate set for validation given the Random Forests algorithm is a classifier based on primarily two methods - bagging and random subspace method.  And the datasets used are created using bootstrapping (resampling with replacement), and internally performs cross-validation to refine the model. The out-of-bag error estimate defines the estimation errors in the internal generated validation sets.   

However, as it is one of the project evaluation criteria, there is no harm to create a cross validation dataset to compare the model created by the training subset.  

```{r}
# Divide the training data into a training set and a validation set
inTrain <- createDataPartition(subTrain$classe, p = 0.6, list = FALSE)
subTraining <- subTrain[inTrain,]
subValidation <- subTrain[-inTrain,]
```

## Create the prediction model (using random forest)
Using the first set of data, to create the prediction model (using random forest).  

Because of the CPU resources required, first, setup to run it in Parallel, using all the CPU cores available...as advised by Course Forum.  Also, even with it running on multiple cores, it will take a long time to create the prediction model. As such, load the model from a previous good run by checking if the model file exists.  
  
```{r}
# Check if model file exists
model <- "modelFit.RData"
if (!file.exists(model)) {

    # If not, set up the parallel clusters.  
    require(parallel)
    require(doParallel)
    cl <- makeCluster(detectCores() - 1)
    registerDoParallel(cl)
    
    fit <- train(subTraining$classe ~ ., method = "rf", data = subTraining)
    save(fit, file = "modelFit.RData")
    
    stopCluster(cl)
} else {
    # Good model exists from previous run, load it and use it.  
    load(file = "modelFit.RData", verbose = TRUE)
}
```

## Measure the Accuracy and Sample Error of the prediction model
Using the training subset and create a prediction.  Then measure it's accuracy

```{r}
predTrain <- predict(fit, subTraining)
confusionMatrix(predTrain, subTraining$classe)
```

Using the validation subset and create a prediction.  Then measure it's accuracy.  From the training subset, the accuracy is very high, at above 99%.  The sample error is 0.0008.

```{r}
predValidation <- predict(fit, subValidation)
confusionMatrix(predValidation, subValidation$classe)
```

From the validation subset, the accuracy is still very high, at above 99%, with an out-of-sample error of 0.0003.  Not significantly different from the sample error.  
  
From the model, the following are the list of important predictors in the model.  

```{r}
varImp(fit)
fit$finalModel
```

The reported OOB Estimated Error is at 12%.  However, based on the validation accuracy at over 99%, with CI between 99.87% to 99.97%, the prediction model can be applied to the final testing set, and predict the classe in the 20 test cases.  

## Apply the prediction model
Apply the prediction model to the testing data. The predicted classification are:  

```{r}
predTesting <- predict(fit, testing)
predTesting
```

And generate the files for submission with the given R code from the assignment.

```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(predTesting)
```

   
   ## End